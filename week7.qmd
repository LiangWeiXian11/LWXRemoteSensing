# Week 7 Classification

## Summary

### Machine learning

Machine learning: science of computer modeling of learning process.

Machine learning is a search through all the data explain the input data can be used on new input data.

Expert systems:

1.  A system that uses human knowledge to solve problems that normally require human intelligence.

2.  Knowledge from expert (experience)

When humans have some generalizations, we can reach a logical assumption or conclusion = **inductive learning**

### Classification and regression trees (CART)

**Decision trees** are hierarchical classifiers used for both [classification and regression]{.underline} tasks.

1.  Classification trees

    1.  classify data into two or more discrete (can only have certain values) categories

    2.  A decision tree can easily be transformed to a set of rules by mapping from the root node to the leaf nodes one by one.

![Rule of Decision Tree. Source: [Decision Tree.](https://www.saedsayad.com/decision_tree.htm)](img/week7/classificationTree.png){width="568"}

```         
3.  Regression trees

    1.  predict continuous dependent variable

    2.  subset the data into [smaller chunks]{.underline}

    Case 1; one line doesn't fit the whole dataset:
```

![Case 1A. Source: Andrew drew it in Lecture 7](img/week7/RegressionC1.png){width="51%"}

![Case 1B. Source: How do Regression Trees Work?](img/week7/RegressionC1B.png){width="56%"}

```         
    Case 2; regression tree; divide to different subset then doing regression on each individual component.
```

![Case 2. Source: [How do Regression Trees Work?](https://medium.datadriveninvestor.com/how-do-regression-trees-work-94999c5105d)](img/week7/RegressionC2.png){width="386"}

```         
    So in this case, the tree uses the average value (100%) as the prediction value for dosages between 14.5 and 23.5.
```

### Overfitting

-   bias: difference between predicted value and true value (oversimplifies model)

-   variance: variability of model for a given point (not genearlise well)

![best model. Source: [Bellini and Cascella, 2022.](https://www.researchgate.net/publication/364826401_Understanding_basic_principles_of_Artificial_Intelligence_a_practical_guide_for_intensivists)](img/week7/overfitting.png){width="662"}

### **Supervised** classification **(**Image)

Main processes:

1.  Class definition
    -   Define the classes or categories into which you want to classify the data. These classes should be mutually exclusive and collectively exhaustive.
    -   Assign labels or class identifiers to the examples in your training dataset.
2.  Pre-processing
    -   Clean the data by handling missing values, outliers, or noise. - Normalize or standardize the features to ensure that they have a similar scale.
    -   Split the data into (training and testing sets)
3.  Training
    -   Select a suitable classification algorithm
    -   Train the model using the labeled examples from the training dataset (the model learns to map input features to the corresponding class labels)
4.  Pixel assignment
    -   pixel assignment involves assigning each pixel in the image to a specific class label based on the trained model's predictions
5.  accuracy assessment
    -   Evaluate the accuracy of the classification model
    -   Compare the model's predictions with the true labels from the test dataset to assess its performance
    -   Adjust the model's parameters or features

**Common methods:**

-   Maximum likelihood

-   SVM (support vector Machine)

    -   Support Vector Machine (SVM) is a supervised learning classification algorithm. which is to find [a middle margin (an optimal separating hyperplane)]{.underline} that separates vertors of different classes while [maximizing the margin]{.underline} of separation. Meanwhile, it is a linear binary classifier that separates training data with the largest possible margin. It places a line (separating hyperplane) between classes, maximizing the distance to the closest points (support vectors).

![SVM. Source: CASA 23 in Lecture 7](img/week7/SVM.png){width="480"}

## Application

### Overall Machine Learning Workflow for Classification (Random Forest Model):

1.  Pre-Processing
    1.  pre-process satellite imagery
    2.  some index for classification such as NDVI, NDWI (those types of landcover are so readily identifiable that we can remove them with thresholds. In this case, we can use these indices to filter out water and vegetation.)
2.  Generating Labeled Data
    1.  Define different land cover classes
    2.  Draw polygons (or points or lines) around the landcover.
    3.  Taking random samples of points from within these polygons
3.  Training a Model by above labeled data.
    1.  **Random Forest classifier**
4.  Validation the model
    1.  Confusion Matrix (Error matrix)

[CASA0025: Building Spatial Applications with Big Data - 7Â  Refinery Identification (oballinger.github.io)](https://oballinger.github.io/CASA0025/W07_refineries.html)

## Reflection

In this week, we learn some Classification methods that slice data differently, aiming to categorize data into classes or values. It also covers some algorithms of **supervised** classification for image such as SVM.

In more detail, the decision between a single decision tree or a forest of trees depends on the desired model complexity and robustness. Hyperparameters govern the behavior of the classifier, influencing its performance and generalization ability.

### New terms:

-   inductive learning: given context we can use experience to make judgement

-   maximum margin classifier(MMC): a machine learning algorithm used for classification tasks.

-   separating hyperplane: a decision boundary that divides the feature space into two regions
